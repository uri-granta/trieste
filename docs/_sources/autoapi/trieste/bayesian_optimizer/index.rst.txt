:mod:`trieste.bayesian_optimizer`
=================================

.. py:module:: trieste.bayesian_optimizer

.. autoapi-nested-parse::

   This module contains the :class:`BayesianOptimizer` class, used to perform Bayesian optimization.



Module Contents
---------------

.. data:: S
   

   Unbound type variable. 


.. data:: SP
   

   Type variable bound to :class:`SearchSpace`. 


.. class:: Record

   Bases: :py:obj:`Generic`\ [\ :py:obj:`S`\ ]

   Container to record the state of each step of the optimization process. 

   .. attribute:: datasets
      :annotation: :collections.abc.Mapping[str, trieste.data.Dataset]

      The known data from the observer. 


   .. attribute:: models
      :annotation: :collections.abc.Mapping[str, trieste.models.TrainableProbabilisticModel]

      The models over the :attr:`datasets`. 


   .. attribute:: acquisition_state
      :annotation: :S | None

      The acquisition state. 


   .. method:: dataset(self) -> trieste.data.Dataset
      :property:

      The dataset when there is just one dataset. 


   .. method:: model(self) -> trieste.models.TrainableProbabilisticModel
      :property:

      The model when there is just one dataset. 



.. class:: OptimizationResult

   Bases: :py:obj:`Generic`\ [\ :py:obj:`S`\ ]

   The final result, and the historical data of the optimization process. 

   .. attribute:: final_result
      :annotation: :trieste.utils.Result[Record[S]]

      The final result of the optimization process. This contains either a :class:`Record` or an
      exception.


   .. attribute:: history
      :annotation: :list[Record[S]]

      The history of the :class:`Record`\ s from each step of the optimization process. These
      :class:`Record`\ s are created at the *start* of each loop, and as such will never include the
      :attr:`final_result`.


   .. method:: astuple(self) -> tuple[trieste.utils.Result[Record[S]], list[Record[S]]]

      **Note:** In contrast to the standard library function :func:`dataclasses.astuple`, this
      method does *not* deepcopy instance attributes.

      :return: The :attr:`final_result` and :attr:`history` as a 2-tuple.


   .. method:: try_get_final_datasets(self) -> collections.abc.Mapping[str, trieste.data.Dataset]

      Convenience method to attempt to get the final data.

      :return: The final data, if the optimization completed successfully.
      :raise Exception: If an exception occurred during optimization.


   .. method:: try_get_final_dataset(self) -> trieste.data.Dataset

      Convenience method to attempt to get the final data for a single dataset run.

      :return: The final data, if the optimization completed successfully.
      :raise Exception: If an exception occurred during optimization.
      :raise ValueError: If the optimization was not a single dataset run.


   .. method:: try_get_final_models(self) -> collections.abc.Mapping[str, trieste.models.TrainableProbabilisticModel]

      Convenience method to attempt to get the final models.

      :return: The final models, if the optimization completed successfully.
      :raise Exception: If an exception occurred during optimization.


   .. method:: try_get_final_model(self) -> trieste.models.TrainableProbabilisticModel

      Convenience method to attempt to get the final model for a single model run.

      :return: The final model, if the optimization completed successfully.
      :raise Exception: If an exception occurred during optimization.
      :raise ValueError: If the optimization was not a single model run.



.. class:: BayesianOptimizer(observer: trieste.observer.Observer, search_space: SP)


   Bases: :py:obj:`Generic`\ [\ :py:obj:`SP`\ ]

   This class performs Bayesian optimization, the data-efficient optimization of an expensive
   black-box *objective function* over some *search space*. Since we may not have access to the
   objective function itself, we speak instead of an *observer* that observes it.

   :param observer: The observer of the objective function.
   :param search_space: The space over which to search. Must be a
       :class:`~trieste.space.SearchSpace`.

   .. method:: optimize(self, num_steps: int, datasets: collections.abc.Mapping[str, trieste.data.Dataset], model_specs: collections.abc.Mapping[str, trieste.models.ModelSpec], *, track_state: bool = True) -> OptimizationResult[None]
               optimize(self, num_steps: int, datasets: collections.abc.Mapping[str, trieste.data.Dataset], model_specs: collections.abc.Mapping[str, trieste.models.ModelSpec], acquisition_rule: trieste.acquisition.rule.AcquisitionRule[S, SP], acquisition_state: S | None = None, *, track_state: bool = True) -> OptimizationResult[S]
               optimize(self, num_steps: int, datasets: trieste.data.Dataset, model_specs: trieste.models.ModelSpec, *, track_state: bool = True) -> OptimizationResult[None]
               optimize(self, num_steps: int, datasets: trieste.data.Dataset, model_specs: trieste.models.ModelSpec, acquisition_rule: trieste.acquisition.rule.AcquisitionRule[S, SP], acquisition_state: S | None = None, *, track_state: bool = True) -> OptimizationResult[S]

      Attempt to find the minimizer of the ``observer`` in the ``search_space`` (both specified at
      :meth:`__init__`). This is the central implementation of the Bayesian optimization loop.

      For each step in ``num_steps``, this method:
          - Finds the next points with which to query the ``observer`` using the
            ``acquisition_rule``'s :meth:`acquire` method, passing it the ``search_space``,
            ``datasets``, models built from the ``model_specs``, and current acquisition state.
          - Queries the ``observer`` *once* at those points.
          - Updates the datasets and models with the data from the ``observer``.

      If any errors are raised during the optimization loop, this method will catch and return
      them instead, along with the history of the optimization process, and print a message (using
      `absl` at level `logging.ERROR`).

      **Note:** While the :class:`~trieste.models.TrainableProbabilisticModel` interface implies
      mutable models, it is *not* guaranteed that the model passed to :meth:`optimize` will
      be updated during the optimization process. For example, if ``track_state`` is `True`, a
      copied model will be used on each optimization step. Use the models in the return value for
      reliable access to the updated models.

      **Type hints:**
          - The ``acquisition_rule`` must use the same type of
            :class:`~trieste.space.SearchSpace` as specified in :meth:`__init__`.
          - The ``acquisition_state`` must be of the type expected by the ``acquisition_rule``.
            Any acquisition state in the optimization result will also be of this type.

      :param num_steps: The number of optimization steps to run.
      :param datasets: The known observer query points and observations for each tag.
      :param model_specs: The model to use for each :class:`~trieste.data.Dataset` in
          ``datasets``.
      :param acquisition_rule: The acquisition rule, which defines how to search for a new point
          on each optimization step. Defaults to
          :class:`~trieste.acquisition.rule.EfficientGlobalOptimization` with default
          arguments. Note that if the default is used, this implies the tags must be
          `OBJECTIVE`, the search space can be any :class:`~trieste.space.SearchSpace`, and the
          acquisition state returned in the :class:`OptimizationResult` will be `None`.
      :param acquisition_state: The acquisition state to use on the first optimization step.
          This argument allows the caller to restore the optimization process from an existing
          :class:`Record`.
      :param track_state: If `True`, this method saves the optimization state at the start of each
          step. Models and acquisition state are copied using `copy.deepcopy`.
      :return: An :class:`OptimizationResult`. The :attr:`final_result` element contains either
          the final optimization data, models and acquisition state, or, if an exception was
          raised while executing the optimization loop, it contains the exception raised. In
          either case, the :attr:`history` element is the history of the data, models and
          acquisition state at the *start* of each optimization step (up to and including any step
          that fails to complete). The history will never include the final optimization result.
      :raise ValueError: If any of the following are true:

          - ``num_steps`` is negative.
          - the keys in ``datasets`` and ``model_specs`` do not match
          - ``datasets`` or ``model_specs`` are empty
          - the default `acquisition_rule` is used and the tags are not `OBJECTIVE`.



